{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a TSV file.\n",
    "    \n",
    "    :param file_path (str): The path to the .tsv file containing the data.\n",
    "    \n",
    "    :return pandas.DataFrame: The loaded dataset.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, sep='\\t')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(data, column, new_column_name):\n",
    "    \"\"\"\n",
    "    Rename a column in a pandas.DataFrame.\n",
    "    \n",
    "    :param data (pandas.DataFrame): The dataset.\n",
    "    :param column (str): The name of the column to rename.\n",
    "    :param new_column_name (str): The new name of the column.\n",
    "    \n",
    "    :return pandas.DataFrame: The dataset with the renamed column.\n",
    "    \"\"\"\n",
    "    data.rename(columns={column: new_column_name}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data_by_toxicity(data, toxicity_column='ref_tox'):\n",
    "    \"\"\"\n",
    "    Sort the dataset by the specified toxicity column.\n",
    "\n",
    "    :param data (pandas.DataFrame): The dataset to sort.\n",
    "    :param toxicity_column (str): The column name of the toxicity to sort by.\n",
    "    \n",
    "    :return pandas.DataFrame: The sorted dataset.\n",
    "    \"\"\"\n",
    "    return data.sort_values(toxicity_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uncertain_toxicity_data(sorted_data, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Remove entries where toxicity is not determined properly by slicing the dataset.\n",
    "\n",
    "    :param sorted_data (pandas.DataFrame): The sorted dataset.\n",
    "    :param lower_bound (int): The lower index to slice from.\n",
    "    :param upper_bound (int): The upper index to slice to.\n",
    "    \n",
    "    :return pandas.DataFrame: The dataset with uncertain toxicity data removed.\n",
    "    \"\"\"\n",
    "    part1 = sorted_data.iloc[:lower_bound]\n",
    "    part2 = sorted_data.iloc[upper_bound:]\n",
    "    return pd.concat([part1, part2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_translation_and_reference(part_data):\n",
    "    \"\"\"\n",
    "    Swap the 'reference' and 'translation' columns in the dataset, and their corresponding toxicity scores.\n",
    "\n",
    "    :param part_data (pandas.DataFrame): The part of the dataset to manipulate.\n",
    "    \n",
    "    :return pandas.DataFrame: The dataset with 'reference' and 'translation' columns swapped.\n",
    "    \"\"\"\n",
    "    ref_temp = part_data['reference'].copy()\n",
    "    ref_value = part_data['ref_tox'].copy()\n",
    "\n",
    "    part_data['reference'] = part_data['translation']\n",
    "    part_data['translation'] = ref_temp\n",
    "    part_data['ref_tox'] = part_data['trn_tox']\n",
    "    part_data['trn_tox'] = ref_value\n",
    "\n",
    "    return part_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_translation_toxicity(data, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Filter rows based on the translation toxicity.\n",
    "\n",
    "    :param data (pandas.DataFrame): The dataset to filter.\n",
    "    :param threshold (float): The toxicity threshold to filter by.\n",
    "    \n",
    "    :return pandas.DataFrame: The filtered dataset.\n",
    "    \"\"\"\n",
    "    return data[data.trn_tox < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data, file_path):\n",
    "    \"\"\"\n",
    "    Save the DataFrame to a CSV file.\n",
    "\n",
    "    :param data (pandas.DataFrame): The dataset to save.\n",
    "    :param file_path (str): The file path to save the dataset to.\n",
    "    \"\"\"\n",
    "    data.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=0.2, valid_size=0.1):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "\n",
    "    :param data (pandas.DataFrame): The dataset to split.\n",
    "    :param test_size (float): The proportion of the dataset to include in the test split.\n",
    "    :param valid_size (float): The proportion of the dataset to include in the validation split.\n",
    "    \n",
    "    :return tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # First split to get the test set\n",
    "    train_valid_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Adjust valid_size to compensate for the initial split\n",
    "    valid_size_adjusted = valid_size / (1 - test_size)\n",
    "\n",
    "    # Now split the remaining data to get the validation set\n",
    "    train_data, valid_data = train_test_split(train_valid_data, test_size=valid_size_adjusted, random_state=42)\n",
    "\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/raw/filtered.tsv'\n",
    "sorted_file_path = '../data/internal/data.csv'\n",
    "train_file_path = '../data/internal/train_data.csv'\n",
    "valid_file_path = '../data/internal/valid_data.csv'\n",
    "test_file_path = '../data/internal/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(file_path)\n",
    "\n",
    "# Rename the first column to 'id'\n",
    "dataset = rename_column(dataset, dataset.columns[0], 'id')\n",
    "\n",
    "# Sort by reference toxicity\n",
    "sorted_data = sort_data_by_toxicity(dataset)\n",
    "\n",
    "# Remove uncertain toxicity data\n",
    "cleaned_data = remove_uncertain_toxicity_data(sorted_data, 200_000, 300_000)\n",
    "\n",
    "# Swap reference and translation\n",
    "cleaned_data = swap_translation_and_reference(cleaned_data)\n",
    "\n",
    "# Filter by translation toxicity\n",
    "filtered_data = filter_by_translation_toxicity(cleaned_data)\n",
    "\n",
    "# Save to new CSV\n",
    "save_data_to_csv(filtered_data, sorted_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_data, valid_data, test_data = split_data(filtered_data)\n",
    "\n",
    "# Save the splits to CSV files\n",
    "save_data_to_csv(train_data, '../data/internal/train_data.csv')\n",
    "save_data_to_csv(valid_data, '../data/internal/valid_data.csv')\n",
    "save_data_to_csv(test_data, '../data/internal/test_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
